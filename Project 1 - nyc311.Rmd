---
title: "Project 1"
author: "Zac Macintyre"
date: "1/27/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
library("RSocrata")

url1 = "https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$where=borough='MANHATTAN' AND created_date between '2014-07-01T00:00:00.000' and '2017-06-30T00:00:00.000'"
df311 <- read.socrata(url1)

```

```{r}
url2 = "https://data.cityofnewyork.us/resource/k397-673e.json?$where=fiscal_year > 2014 AND fiscal_year < 2018"
city_pay = read.socrata(url2)
```
##### DO NOT TOUCH ABOVE CODE #######
#### IT TAKES FOREVER TO LOAD ######


So, all the code below did not really lead to any insights.  I am not sure when you are going to look at it, but this is all I will get done tonight.  I need to think about how else to analyze this to get something actionable.


```{r}
library(plyr)
library(dplyr)

#function that converted pay data from char into double values
to_double = function(df) {
  df = df %>% mutate(base_salary = as.double(base_salary), regular_hours = as.double(regular_hours),
                           ot_hours = as.double(ot_hours), total_ot_paid = as.double(total_ot_paid),
                           total_other_pay = as.double(total_other_pay), 
                           regular_gross_paid = as.double(regular_gross_paid))
  return(df)
}

city_pay = to_double(city_pay)
```

```{r}
#getting initial samples 
index = sample(nrow(city_pay), 10000)
samp_city_pay = city_pay[index,]
head(samp_city_pay,10)

index = sample(nrow(df311), 10000)
s311 = df311[index,]
head(s311)


#looked up nyc city fiscal year, it starts july 1st
#want to make a function that takes the date and just gives back the year
to_year = function(df) {
  
  df$year = NA
  for(i in 1:length(df$year)) {
    year = df$created_date[i]
    
      if (!is.na(year) & year > as.POSIXct("2017-06-30", format = "%Y-%m-%d") & 
      year < as.POSIXct("2018-06-30", format = "%Y-%m-%d")) {
    df$year[i] = 2018
  
    } else if(!is.na(year) & year > as.POSIXct("2016-06-30", format = "%Y-%m-%d") & 
            year < as.POSIXct("2017-06-30", format = "%Y-%m-%d")) {
    df$year[i] = 2017
  
    } else if (!is.na(year) & year > as.POSIXct("2015-06-30", format = "%Y-%m-%d") & 
               year < as.POSIXct("2016-06-30", format = "%Y-%m-%d")){
    df$year[i] = 2016
  
    } else if (!is.na(year) & year > as.POSIXct("2014-06-30", format = "%Y-%m-%d") & 
               year < as.POSIXct("2015-06-30", format = "%Y-%m-%d"))
    df$year[i] = 2015
  }
  return(df)
}



```


```{r}
unique(city_pay$fiscal_year)

#making year DFs just to see some data year by year easily 
year_2015 = city_pay[city_pay$fiscal_year == 2015,]
year_2016 = city_pay[city_pay$fiscal_year == 2016,]
year_2017 = city_pay[city_pay$fiscal_year == 2017,]

```


This is the last thing I updated, maybe it is something to look more in-depth at 
```{r}
#section to compare some features of these years 

#667,305,085 Million dollar difference between 2017-1016 
sum(year_2017$base_salary + year_2017$total_ot_paid) - sum(year_2016$base_salary + year_2016$total_ot_paid) 

#1,061,901,727 Billion dollar difference between 2016-2015
sum(year_2016$base_salary + year_2016$total_ot_paid) - sum(year_2015$base_salary + year_2015$total_ot_paid)

#562266
length(year_2017$agency_name)

#544817
length(year_2016$agency_name)

#577880
length(year_2015$agency_name)

year_2015$total_pay = year_2015$base_salary + year_2015$total_ot_paid
year_2016$total_pay = year_2016$base_salary + year_2016$total_ot_paid
year_2017$total_pay = year_2017$base_salary + year_2017$total_ot_paid

```

Trying to explore the data differently.  I am going top down, seeing who the upper 16th qu is and seeing what those places do
```{r}
#3rd Qu = 76706,  max = 300,000,   upper 16th = 106537
summary(year_2015[year_2015$total_pay > 76706, "total_pay"])

#3rd Qu = 81254,  max = 350,000,   upper 16th = 112926
summary(year_2016[year_2016$total_pay > 81254, "total_pay"])

#3rd Qu = 82805,  max = 350,000,  upper 16th = 115605
summary(year_2017[year_2017$total_pay > 82805, "total_pay"])

```
The original DF was very long tried to make it much smaller 
```{r}
rich_2015 = year_2015[year_2015$total_pay > 76706, ]

rich_2016 = year_2016[year_2016$total_pay > 81254, ]

rich_2017 = year_2017[year_2017$total_pay > 82805, ]


rich = plyr::rbind.fill(rich_2015, rich_2016, rich_2017)

rich$full_name = paste(rich$first_name, rich$last_name)


rich[rich$total_pay > 250000 & rich$work_location_borough == "MANHATTAN",]
```
Above didnt do much for me.  Now on too looking at those most complaint types throughout the years 

```{r}
year_20
```


```{r}
#end of the night thoughts to maybe do something with 2moro
hist(samp_city_pay$ot_hours)
overtime = samp_city_pay[samp_city_pay$ot_hours > 100,]
table(overtime$agency_name)

#converting the years
s311 = to_year(s311)

#getting dimensions of my samples
dim(s311)
dim(samp_city_pay)


# getting complaint types where on average there is at least 1 complaint every other day
complaints = table(s311$complaint_type) > 183
complaints = names(complaints)[complaints == T]
as.vector(complaints)

#getting depts that have high complaint rates 
new = s311[s311$complaint_type %in% complaints, ]
dept = toupper(unique(new$agency_name))
sort(unique(city_pay$agency_name))
dept

dept1 = c("DEPARTMENT OF TRANSPORTATION", "POLICE DEPARTMENT", "DEPARTMENT OF BUILDINGS", "DEPT. OF HOMELESS SERVICES", 
          "TAXI & LIMOUSINE COMMISSION", "DEPT OF ENVIRONMENT PROTECTION", "HOUSING PRESERVATION & DVLPMNT")

#called it new city 
new_city = city_pay[city_pay$agency_name %in% dept1, ]

head(new_city)
```

```{r}
#creating a DF to start with
agency = table(s311$agency)
agency_name = table(s311$agency_name)

agency = as.data.frame(as.list(agency))
agency_name = as.data.frame(as.list(agency_name))

agency$overtime = sum(samp_city_pay$ot_hours)
agency$pay = sum(samp_city_pay[samp_city_pay$work_location_borough == "MANHATTAN", "base_salary"]) + sum(samp_city_pay[samp_city_pay$work_location_borough == "MANHATTAN", "total_ot_paid"])

head(agency)
```


```{r}
#updating the DF to have N values 
N = 250
for (i in 1:N) {

  index = sample(nrow(city_pay), 10000)
  samp_city_pay = city_pay[index,]
    
  index = sample(nrow(df311), 10000)
  s311 = df311[index,]
    
  temp_agency = table(s311$agency)
  temp_agency = as.data.frame(as.list(temp_agency))
    
  temp_agency$overtime = sum(samp_city_pay$ot_hours)
  temp_agency$pay = sum(samp_city_pay[samp_city_pay$work_location_borough == "MANHATTAN", "base_salary"])
  + sum(samp_city_pay[samp_city_pay$work_location_borough == "MANHATTAN", "total_ot_paid"])
    
  agency = plyr::rbind.fill(agency, temp_agency)
    
}

dim(agency)
head(agency)

```

```{r}
#want to create a scaler, this just normalizes the data to ~N(0,1) 
scaler = function(x) {
  mu = mean(x, na.rm = T)
  deviation = sd(x, na.rm = T)
  return((x - mu) / deviation)
}

```


```{r}
#getting my Y's 
y1 = scaler(agency$overtime)
y2 = scaler(agency$pay)


#setting my Xs
x1 = scaler(agency$X3.1.1)
x2 = scaler(agency$DCA)
x3 = scaler(agency$DEP)
x4 = scaler(agency$DFTA)
x5 = scaler(agency$DHS)
x6 = scaler(agency$DOB)
x7 = scaler(agency$DOE)
x8 = scaler(agency$DOF)
x9 = scaler(agency$DOHMH)
x10 = scaler(agency$DOITT)
x11 = scaler(agency$DOT)
x12 = scaler(agency$DPR)
x13 = scaler(agency$DSNY)
x14 = scaler(agency$EDC)
x15 = scaler(agency$HPD)
x16 = scaler(agency$NYPD)



#nothing really happened here I got x2 was significant at .05
model = lm(y2 ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16)
summary(model)

```

Scaling did nothing to help me.  Below I have visualized the data and it is random, a residual plot should look like this.  I am going to see if another methods help find significance in the data  

```{r}
#I am a bit worried that the plot is random and that the we need to get differnt values for y1 or y2
plot(y2)
```
```{r}
#getting my Y's 
y1 = agency$overtime
y2 = agency$pay


#setting my Xs
x1 = agency$X3.1.1
x2 = agency$DCA
x3 = agency$DEP
x4 = agency$DFTA
x5 = agency$DHS
x6 = agency$DOB
x7 = agency$DOE
x8 = agency$DOF
x9 = agency$DOHMH
x10 = agency$DOITT
x11 = agency$DOT
x12 = agency$DPR
x13 = agency$DSNY
x14 = agency$EDC
x15 = agency$HPD
x16 = agency$NYPD

#just normal lm()
#x2 was significant again at .05
model = lm(y2 ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16)
summary(model)
```



I m trying to see what a correlation matrix tells me about the data

```{r}
#getting a correlation matrix
agency_mat = as.matrix(agency)

cor_agency_mat = cor(agency_mat, use='pairwise.complete.obs')
```


```{r}
#plotting correlations
lower = lower.tri(cor_agency_mat)

hist(cor_agency_mat[lower], xlab = 'correlations of lower matrix')
```
The data is pretty non-correlated.  My guess is we have to add different criteria to this data to help get anything.  Just using Dept names for who responded to 311 calls did not really help anything.  Maybe there is something there that pretty much none of them have anything to do with total pay.  My guess is that the way I sampled total pay randomized it too much.  In other words, if we try to use this we need t ocontrol for year or something.













