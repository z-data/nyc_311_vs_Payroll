---
title: "Project 1"
author: "Zac Macintyre"
date: "1/27/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
library("RSocrata")

url1 = "https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$where=borough='MANHATTAN' AND created_date between '2014-07-01T00:00:00.000' and '2017-06-30T00:00:00.000'"
df311 <- read.socrata(url1)

```

```{r}
url2 = "https://data.cityofnewyork.us/resource/k397-673e.json?$where=fiscal_year > 2014 AND fiscal_year < 2018"
city_pay = read.socrata(url2)
```
##### DO NOT TOUCH ABOVE CODE #######
#### IT TAKES FOREVER TO LOAD ######


```{r}
library(plyr)
library(dplyr)

#function that converted pay data from char into double values
to_double = function(df) {
  df = df %>% mutate(base_salary = as.double(base_salary), regular_hours = as.double(regular_hours),
                           ot_hours = as.double(ot_hours), total_ot_paid = as.double(total_ot_paid),
                           total_other_pay = as.double(total_other_pay), 
                           regular_gross_paid = as.double(regular_gross_paid))
  return(df)
}

city_pay = to_double(city_pay)

city_pay = city_pay[city_pay$work_location_borough == "MANHATTAN",]
city_pay = city_pay[city_pay$fiscal_year == 2017,]


```
Dont think this actually came up as super useful for our analysis 
```{r}
#looked up nyc city fiscal year, it starts july 1st
#want to make a function that takes the date and just gives back the year
to_year = function(df) {
  
  df$year = NA
  for(i in 1:length(df$year)) {
    year = df$created_date[i]
    
      if (!is.na(year) & year > as.POSIXct("2017-06-30", format = "%Y-%m-%d") & 
      year < as.POSIXct("2018-06-30", format = "%Y-%m-%d")) {
    df$year[i] = 2018
  
    } else if(!is.na(year) & year > as.POSIXct("2016-06-30", format = "%Y-%m-%d") & 
            year < as.POSIXct("2017-06-30", format = "%Y-%m-%d")) {
    df$year[i] = 2017
  
    } else if (!is.na(year) & year > as.POSIXct("2015-06-30", format = "%Y-%m-%d") & 
               year < as.POSIXct("2016-06-30", format = "%Y-%m-%d")){
    df$year[i] = 2016
  
    } else if (!is.na(year) & year > as.POSIXct("2014-06-30", format = "%Y-%m-%d") & 
               year < as.POSIXct("2015-06-30", format = "%Y-%m-%d"))
    df$year[i] = 2015
  }
  return(df)
}
```

Section of functions I use for apply methods later on
```{r}
#function for calculating the mean of total pay
new_mean = function(df) {
  return( mean(df$total_pay, na.rm = T))
}
#function for calculating the total complaints per dept 
new_sum = function(df) {
  return(length(df$complaint_type))
}

#new max just returns the highest pay from any of the targeted dept
new_max = function(df) {
  return( max(df$total_pay))
}

#emps returns the total number of employees in each dept 
emps = function(df) {
  return(length(df$total_pay))
}

#gets budgets 
budget = function(df) {
  return( sum(df$total_pay, na.rm = T))
}
```

Setting the major departments percentage of total budget that we use to regress on
```{r}
#overall budgets from the years on average
city = (84095875859 + 80538508607 +78581696096)/3
nypd = ((4896334549 + 5075080640 + 5312163257)/3)/city
DOT = 1000000000/city
environ = 1450000000/city
mental  = 1490000000/city
housing = 1100000000/city
building = 155400000/city
homeless = 20600000/city
taxi = 50200000/city
terms = c()
```

Getting an initial sample of each data set.  I thought 10,000 samples would be sufficient for good analyses 
```{r}
#getting initial samples 
index = sample(nrow(city_pay), 10000)
samp_city_pay = city_pay[index,]
#head(samp_city_pay)

index = sample(nrow(df311), 10000)
s311 = df311[index,]
#head(s311)
```
There are a little over 1000 complaints a day on average in Manhattan.  At 10,000 samples that is close to 10 days of random samples.  There we have complaints that come in at over 15 times a day in those samples with the complaints being above 150.  We repeat this 10 times to get 100 random days throughout the year.  This out put will give us dept names and the frequency with which they happened in each of the 10 times the for loop ran
```{R}
N = 10
dept_comp = c()
comps = c()
for (i in 1:N) {
  
  index = sample(nrow(df311), 10000)
  s311 = df311[index,]
  
  complaints_hist = table(s311$complaint_type)
  complaints = table(s311$complaint_type) > 150
  complaints = names(complaints)[complaints == T]
  complant_det = unique(s311[s311$complaint_type %in% complaints, "agency_name"])
  
  comps = append(comps, complaints)
  dept_comp = append(dept_comp, complant_det)
  
}

#table(dept_comp)
dept_names = unique(dept_comp)

#table(comps)
comp_names = unique(comps)

```

Engineered the new feature of the data.  it is average salary of the major departments / total complaints the department had in the sample 
```{r}

#getting names for all the deptartments, had to do by hand because they do not match in each DF
dept1 = c("DEPARTMENT OF BUILDINGS", "DEPT OF ENVIRONMENT PROTECTION", "DEPT OF HEALTH/MENTAL HYGIENE", "HOUSING PRESERVATION & DVLPMNT",
          "DEPARTMENT OF TRANSPORTATION", "POLICE DEPARTMENT", "DEPT. OF HOMELESS SERVICES", "TAXI & LIMOUSINE COMMISSION")

#creating a new DF based off the sample and the important depts
new_city = city_pay[city_pay$agency_name %in% dept1, ]
new_city$total_pay = new_city$base_salary + new_city$total_ot_paid

#getting depts that have high complaint rates 
new = s311[s311$complaint_type %in% comp_names, ]
dept = toupper(unique(new$agency_name))
#sort(unique(city_pay$agency_name))
#dept

#splitting the City Pay data by dept and getting the mean pay
new_city.split = split(new_city, new_city$agency_name)
mu = sapply(new_city.split, new_mean)

#splitting the 311 by dept and getting the total complaints
new311.split = split(new, new$agency_name)
total = sapply(new311.split, new_sum)


#removing the depts that fall under larger depts 
if ( !is.na(total["Traffic Management Center"]) ) {
  total["New York City Police Department"] = total["New York City Police Department"] + total["Traffic Management Center"]
  for (i in 1:length(names(total))) {
    if (names(total)[i] == "Traffic Management Center") {
      print('TMC')
      total = total[-(i)]
    }
  }
}

if ( !is.na(total["Division of Alternative Management"]) ) {
  total["Department of Housing Preservation and Development"] = total["Department of Housing Preservation and Development"] + total["Division of Alternative Management"]
  for (i in 1:length(total)) {
    if (names(total)[i] == "Division of Alternative Management") {
      total = total[-i]
    }
  }
}

if ( !is.na(total["DOT"]) ) {
  total["Department of Transportation"] = total["Department of Transportation"] + total["DOT"]
  for (i in 1:length(names(total))) {
    if (names(total)[i] == "DOT") {
       print('DOT')
      total = total[-i]
    }
  }
}

#removing the depts that fall under larger depts 
if ( !is.na(total["NYPD"]) ) {
  total["New York City Police Department"] = total["New York City Police Department"] + total["NYPD"]
  for (i in 1:length(names(total))) {
    if (names(total)[i] == "NYPD") {
       print('NYPD')
      total = total[-(i)]
    }
  }
}

#initializing the pay per complaint and changing the names in the total to be the same as mu
names(total) = dept1
per_comp = total

#for loop to get the ratio
for (i in dept1) {
  per_comp[i] = mu[i]/total[i]
}

#making it a DF so it easy to manage
per_comp = as.data.frame(t(per_comp))
per_comp

```
Adding additional values to the DF 
```{r}
per_comp = plyr::rbind.fill(per_comp, as.data.frame(t(total)), as.data.frame(t(mu)))
#rownames(per_comp) = c("per complaint", "total complaints", "average salary")

#per_comp
```
Here I engineered another feature, which is called factor.  It is the number of complaints a certain department receives that fall under complaint types with at least 150 complaints in the sample.  So the factor is how many major complaints each dept has to deal with
```{r}
# i used this to determine a "factor"  this is another new feature
#it is which departments the common complaints belong too so they more common complaints the higher the factor 
dept = c()
for (i in comp_names) {
  d = unique(new311[new311$complaint_type == i, "agency_name"])
  dept = append(dept, d)
}
dept = table(dept)


#removing the depts that fall under larger depts 
if ( !is.na(dept["Traffic Management Center"]) ) {
  dept["New York City Police Department"] = dept["New York City Police Department"] + dept["Traffic Management Center"]
  for (i in 1:length(names(dept))) {
    if (names(dept)[i] == "Traffic Management Center") {
      dept[-(i)]
    }
  }
}


if ( !is.na(dept["Division of Alternative Management"]) ) {
  dept["Department of Housing Preservation and Development"] = dept["Department of Housing Preservation and Development"] + dept["Division of Alternative Management"]
  for (i in 1:length(names(dept))) {
    if (names(dept)[i] == "Division of Alternative Management") {
      dept[-i]
    }
  }
}

if ( !is.na(dept["DOT"]) ) {
  dept["Department of Transportation"] = dept["Department of Transportation"] + dept["DOT"]
  for (i in 1:length(names(dept))) {
    if (names(dept)[i] == "DOT") {
      dept[-i]
    }
  }
}

if ( !is.na(dept["NYPD"]) ) {
  dept["New York City Police Department"] = dept["New York City Police Department"] + dept["NYPD"]
  for (i in 1:length(names(dept))) {
    if (names(dept)[i] == "NYPD") {
      dept[-i]
    }
  }
}



#creating a budget percentage vector to add to the DF
budget = c(building, environ, mental, housing, DOT, nypd, homeless, taxi)

#Adding the factor to the DF
per_comp = rbind(per_comp, dept)

#getting the returns on the defined functions above
high = sapply(new_city.split, new_max)
employee = sapply(new_city.split, emps)


#combing the above results into the DF
per_comp = plyr::rbind.fill(per_comp, as.data.frame(t(high)), as.data.frame(t(employee)))
per_comp = rbind(per_comp, budget)

#renaming the rows of the DF
#maybe a better way I do not know 
rownames(per_comp) = c("per complaint", "total complaints", "average salary" , "factor", "max salary", "employee count", "budget")

#I want the row names to be what I run the regression on and it is easier this way
#I also scaled down because values are vastly different 
per_comp = as.data.frame(t(per_comp))
scale_comp = as.data.frame(scale(per_comp))
cor(per_comp, use = "complete.obs")
```
Here is our interesting finding within the data 
```{r}
summary(lm(scale_comp$`budget` ~ ., data = scale_comp))

#summary(lm(scale_comp$`budget` ~ scale_comp$factor+scale_comp$`per complaint`+ scale_comp$`max salary`, data = scale_comp))
```
I ran this code by hand to get 5 other samples of the coefficient.  For some reason the code works well in a single run, but in a loop the If statements trip it up.  I think it is because I am changinging the value of the tables as I am iterating over them.  
```{r}
model = summary(lm(scale_comp$`budget` ~ ., data = scale_comp))
terms = append(terms ,coef(model)[,1])
```

A plot of the coeeficient values, they are all negative.  In total this would be equivalant to 500 days of random sampling 
```{r}
#plotting coefficient values
value = c()
for (i in 1:5) {
  v = 5 + 7*(i-1)
  value = append(value, terms[v])
}

plot(value, ylab = "Coefficient of Factor", ylab = "Expiriments Ran", main = "Factor Has a Negative Coefficient")
```
##Code From Here is other exploratory stuff, not used in the project

Just checking other regressions  
```{r}
#this summary is about employee vs 
summary(lm(scale_comp$`employee count` ~ ., data = scale_comp))
```
I know these t values arent that great, but listening to what Wayne said today, this one is super interesting.  Total Complaints has a negative coefficient.  So as complaints go up, the amount of people in the dept is supposed to go down.  It is very interesting.  Especially when looking at other things like average salary.  As average salary goes up you expect more people to be in the dept.  i guess that makes sense, people want to earn a better living.

# When I rerun the data, the coefficient of total complaints become positive. Now I think 
# this is just because the randomness of our sampling process. 
```{r}
summary(lm(scale_comp$`total complaints` ~ ., data = scale_comp))
```

```{r}
summary(lm(scale_comp$factor ~ ., data = scale_comp))
```
```{r}
summary(lm(scale_comp$`per complaint` ~ ., data = scale_comp))
```
```{r}
summary(lm(scale_comp$`average salary` ~ ., data = scale_comp))
```
```{r}
summary(lm(scale_comp$`max salary` ~ ., data = scale_comp))
```
Again just more code that I was looking into getting insights.  It just shows the process 
```{r}
#making year DFs just to see some data year by year easily 
year_2015 = city_pay[city_pay$fiscal_year == 2015,]
year_2016 = city_pay[city_pay$fiscal_year == 2016,]
year_2017 = city_pay[city_pay$fiscal_year == 2017,]
```

I had an idea to look at the pay data by year and maybe learn something.  I havent deleted any code, that I think might still somehow be useful
```{r}
#section to compare some features of these years 

#667,305,085 Million dollar difference between 2017-1016 
sum(year_2017$base_salary + year_2017$total_ot_paid) - sum(year_2016$base_salary + year_2016$total_ot_paid) 

#1,061,901,727 Billion dollar difference between 2016-2015
sum(year_2016$base_salary + year_2016$total_ot_paid) - sum(year_2015$base_salary + year_2015$total_ot_paid)

#562266
length(year_2017$agency_name)

#544817
length(year_2016$agency_name)

#577880
length(year_2015$agency_name)

year_2015$total_pay = year_2015$base_salary + year_2015$total_ot_paid
year_2016$total_pay = year_2016$base_salary + year_2016$total_ot_paid
year_2017$total_pay = year_2017$base_salary + year_2017$total_ot_paid

```

Trying to explore the data differently.  I am going top down, seeing who the upper 16th qu is and seeing what those places do
```{r}
#3rd Qu = 76706,  max = 300,000,   upper 16th = 106537
summary(year_2015[year_2015$total_pay > 76706, "total_pay"])

#3rd Qu = 81254,  max = 350,000,   upper 16th = 112926
summary(year_2016[year_2016$total_pay > 81254, "total_pay"])

#3rd Qu = 82805,  max = 350,000,  upper 16th = 115605
summary(year_2017[year_2017$total_pay > 82805, "total_pay"])

```

The original DF was very long tried to make it much smaller 
```{r}
rich_2015 = year_2015[year_2015$total_pay > 76706, ]

rich_2016 = year_2016[year_2016$total_pay > 81254, ]

rich_2017 = year_2017[year_2017$total_pay > 82805, ]


rich = plyr::rbind.fill(rich_2015, rich_2016, rich_2017)

rich$full_name = paste(rich$first_name, rich$last_name)


rich[rich$total_pay > 250000 & rich$work_location_borough == "MANHATTAN",]
```
Above didnt do much for me.  Now on too looking at those most complaint types throughout the years 


```{r}
#end of the night thoughts to maybe do something with 2moro
hist(samp_city_pay$ot_hours)
overtime = samp_city_pay[samp_city_pay$ot_hours > 100,]
table(overtime$agency_name)

#converting the years
s311 = to_year(s311)

#getting dimensions of my samples
dim(s311)
dim(samp_city_pay)




mean(new_city[new_city$agency_name == "POLICE DEPARTMENT", "base_salary"])

mean(new_city[new_city$agency_name == "POLICE DEPARTMENT" & new_city$ot_hours > 0, "total_ot_paid"])
```


This code is all about trying to set up a linear regression model with agency being what I want to regress pay against.  It didnt result really anything of significance, but I think this is because they way I got pay isnt right.  i randomly drew 10,000 employees and got their total pay.  I think we need to correlate the pay more to the important departments
```{r}
#creating a DF to start with
agency = table(s311$agency)
agency_name = table(s311$agency_name)

agency = as.data.frame(as.list(agency))
agency_name = as.data.frame(as.list(agency_name))

agency$overtime = sum(samp_city_pay$ot_hours)
agency$pay = sum(samp_city_pay[samp_city_pay$work_location_borough == "MANHATTAN", "base_salary"]) + sum(samp_city_pay[samp_city_pay$work_location_borough == "MANHATTAN", "total_ot_paid"])

head(agency)
```


```{r}
#updating the DF to have N values 
N = 250
for (i in 1:N) {

  index = sample(nrow(city_pay), 10000)
  samp_city_pay = city_pay[index,]
    
  index = sample(nrow(df311), 10000)
  s311 = df311[index,]
    
  temp_agency = table(s311$agency)
  temp_agency = as.data.frame(as.list(temp_agency))
    
  temp_agency$overtime = sum(samp_city_pay$ot_hours)
  temp_agency$pay = sum(samp_city_pay[samp_city_pay$work_location_borough == "MANHATTAN", "base_salary"])
  + sum(samp_city_pay[samp_city_pay$work_location_borough == "MANHATTAN", "total_ot_paid"])
    
  agency = plyr::rbind.fill(agency, temp_agency)
    
}

dim(agency)
head(agency)

```

```{r}
#want to create a scaler, this just normalizes the data to ~N(0,1) 
scaler = function(x) {
  mu = mean(x, na.rm = T)
  deviation = sd(x, na.rm = T)
  return((x - mu) / deviation)
}

```


```{r}
#getting my Y's 
y1 = scaler(agency$overtime)
y2 = scaler(agency$pay)


#setting my Xs
x1 = scaler(agency$X3.1.1)
x2 = scaler(agency$DCA)
x3 = scaler(agency$DEP)
x4 = scaler(agency$DFTA)
x5 = scaler(agency$DHS)
x6 = scaler(agency$DOB)
x7 = scaler(agency$DOE)
x8 = scaler(agency$DOF)
x9 = scaler(agency$DOHMH)
x10 = scaler(agency$DOITT)
x11 = scaler(agency$DOT)
x12 = scaler(agency$DPR)
x13 = scaler(agency$DSNY)
x14 = scaler(agency$EDC)
x15 = scaler(agency$HPD)
x16 = scaler(agency$NYPD)



#nothing really happened here I got x2 was significant at .05
model = lm(y2 ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16)
summary(model)

```

Scaling did nothing to help me.  Below I have visualized the data and it is random, a residual plot should look like this.  I am going to see if another methods help find significance in the data  

```{r}
#I am a bit worried that the plot is random and that the we need to get differnt values for y1 or y2
plot(y2)
```
```{r}
#getting my Y's 
y1 = agency$overtime
y2 = agency$pay


#setting my Xs
x1 = agency$X3.1.1
x2 = agency$DCA
x3 = agency$DEP
x4 = agency$DFTA
x5 = agency$DHS
x6 = agency$DOB
x7 = agency$DOE
x8 = agency$DOF
x9 = agency$DOHMH
x10 = agency$DOITT
x11 = agency$DOT
x12 = agency$DPR
x13 = agency$DSNY
x14 = agency$EDC
x15 = agency$HPD
x16 = agency$NYPD

#just normal lm()
#x2 was significant again at .05
model = lm(y2 ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16)
summary(model)
```



I m trying to see what a correlation matrix tells me about the data
```{r}
#getting a correlation matrix
agency_mat = as.matrix(agency)

cor_agency_mat = cor(agency_mat, use='pairwise.complete.obs')
```


```{r}
#plotting correlations
lower = lower.tri(cor_agency_mat)

hist(cor_agency_mat[lower], xlab = 'correlations of lower matrix')
```
The data is pretty non-correlated.  My guess is we have to add different criteria to this data to help get anything.  Just using Dept names for who responded to 311 calls did not really help anything.  Maybe there is something there that pretty much none of them have anything to do with total pay.  My guess is that the way I sampled total pay randomized it too much.  In other words, if we try to use this we need t ocontrol for year or something.


This code here was used as part of the visualization 
```{r visualization}
index <- sample(nrow(df311),10000)
zipcode_311 <- df311[index,]
temp_1 <- zipcode_311[1,]
query <- list(lon=temp_1$longitude,
              lat=temp_1$latitude)
photon_resp <- httr::GET(endpoint, query=query)
photon_data <- httr::content(photon_resp)
temp_1$zipcode <- photon_data[["features"]][[1]][["properties"]][["postcode"]]
temp_1$zipcode

for (i in 2:nrow(zipcode_311)) {
  temp <- zipcode_311[i,]
  query <- list(lon=temp$longitude,
                lat=temp$latitude)
  photon_resp <- httr::GET(endpoint, query=query)
  photon_data <- httr::content(photon_resp)
  temp$zip_code <- photon_data[["features"]][[1]][["properties"]][["postcode"]]
  temp_1 <- plyr::rbind.fill(temp_1,temp)
}

write.csv(temp_1,'~/Downloads/data_311.csv')
```



```{r}
df = fread('data_311.csv')

zipcode_str = as.character(unique(df$zip_code)[order(unique(df$zip_code))])

char_zips_1 <- zctas(cb = TRUE, starts_with = zipcode_str)


df$zip_code = as.character(df$zip_code)

df_zip = df[, .(.N), by=zip_code]
# Join Data
# join zip boundaries and number of crime 
char_zips_1 <- geo_join(char_zips_1, 
                      df_zip, 
                      by_sp = "GEOID10", 
                      by_df = "zip_code",
                      how = "left")
pal_1 <- colorNumeric(
  palette = "Reds",
  domain = char_zips_1$N)

# create labels for zipcodes
labels <- 
  paste0(
    "Zip Code: ",
    char_zips_1$GEOID10, "<br/>",
    "# of Crime: ",
    char_zips_1$N) %>%
  lapply(htmltools::HTML)


# Make a Map!
char_zips_1 %>% 
  leaflet %>% 
  # add base map
  addProviderTiles("CartoDB") %>% 
  # add zip codes
  addPolygons(fillColor = ~pal_1(N),
              weight = 2,
              opacity = 1,
              color = "white",
              dashArray = "3",
              fillOpacity = 0.7,
              highlight = highlightOptions(weight = 2,
                                           color = "#666",
                                           dashArray = "",
                                           fillOpacity = 0.7,
                                           bringToFront = TRUE),
              label = labels) %>%
  # add legend
  addLegend(pal = pal_1,
            values = ~N,
            opacity = 0.7,
            title = htmltools::HTML("# of crime <br>
                                    by Zip Code <br>
                                    "),
            position = "bottomright")
```






